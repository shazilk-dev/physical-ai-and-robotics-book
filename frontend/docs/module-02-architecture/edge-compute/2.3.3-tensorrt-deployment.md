---
sidebar_position: 3
title: 2.3.3 TensorRT Deployment
---

# 2.3.3 TensorRT Deployment - Optimizing Deep Learning Inference

## Introduction

**Problem:** Your PyTorch YOLOv8 model runs at 20 FPS on Jetson. You need 60 FPS for real-time robotics.

**Solution:** **NVIDIA TensorRT** automatically optimizes models through:
- Layer fusion (fewer kernel launches)
- Precision calibration (FP16/INT8 quantization)
- Kernel auto-tuning (choose fastest implementation)

**Result:** 3-5Ã— speedup with **zero accuracy loss** (for FP16) or <2% loss (for INT8)!

In this section, you'll learn how to convert PyTorch/ONNX models to TensorRT and deploy them on Jetson.

---

## What is TensorRT?

### High-Level Overview

**TensorRT:** NVIDIA's inference optimization SDK that converts trained models into highly optimized runtime engines.

**Pipeline:**
```
PyTorch Model (.pt)
     â†“
ONNX Export (.onnx)
     â†“
TensorRT Builder (optimization)
     â†“
TensorRT Engine (.trt) â† This runs FAST on Jetson!
     â†“
Inference (Python or C++)
```

---

### Key Optimization Techniques

**1. Layer Fusion**

**Before TensorRT:**
```
Conv â†’ ReLU â†’ BatchNorm â†’ Pooling
  â†“      â†“        â†“          â†“
4 separate CUDA kernels (slow!)
```

**After TensorRT:**
```
Conv+ReLU+BatchNorm+Pooling (fused into 1 kernel)
  â†“
Single CUDA kernel (4Ã— faster!)
```

---

**2. Precision Calibration**

**FP32 (Full Precision):**
```
Weight: 1.234567890123 (32 bits per number)
Memory: 100 MB for 25M parameters
Speed: 1.0Ã— baseline
```

**FP16 (Half Precision):**
```
Weight: 1.234 (16 bits per number)
Memory: 50 MB (2Ã— less)
Speed: 2Ã— faster (Tensor cores accelerated!)
Accuracy loss: < 0.1% (negligible)
```

**INT8 (8-bit Integer):**
```
Weight: 78 (8 bits per number)
Memory: 25 MB (4Ã— less)
Speed: 4Ã— faster (INT8 Tensor cores!)
Accuracy loss: 1-2% (acceptable for most tasks)
```

---

**3. Kernel Auto-Tuning**

TensorRT benchmarks multiple implementations of each layer and picks the fastest:

```
Convolution layer implementations:
- Algorithm 1: Direct convolution â†’ 5.2 ms
- Algorithm 2: Im2Col + GEMM â†’ 3.8 ms
- Algorithm 3: Winograd transform â†’ 2.1 ms â† TensorRT picks this!
```

**Result:** Each layer uses optimal algorithm for your specific hardware (Jetson Orin).

---

## TensorRT Workflow

### Step 1: Export PyTorch Model to ONNX

**Example: YOLOv8**

```python
from ultralytics import YOLO

# Load PyTorch model
model = YOLO('yolov8n.pt')

# Export to ONNX
model.export(format='onnx', simplify=True, opset=12)

# Output: yolov8n.onnx (ready for TensorRT!)
```

**Why ONNX?**
- Open standard (works with PyTorch, TensorFlow, etc.)
- TensorRT natively supports ONNX import
- Allows validation before TensorRT conversion

---

### Step 2: Convert ONNX to TensorRT (FP16)

**Using `trtexec` (Command-Line Tool):**

```bash
# FP16 precision (2Ã— speedup, negligible accuracy loss)
/usr/src/tensorrt/bin/trtexec \
    --onnx=yolov8n.onnx \
    --saveEngine=yolov8n_fp16.trt \
    --fp16 \
    --workspace=4096 \
    --verbose

# Output:
# [I] Engine built in 45.2 seconds
# [I] Average latency: 12.3 ms (vs 25 ms PyTorch!)
```

**Key Flags:**
- `--fp16`: Enable FP16 precision (Tensor core acceleration)
- `--workspace=4096`: Allow 4 GB temp memory for optimization
- `--verbose`: Show detailed build log

---

### Step 3: Inference with TensorRT (Python)

**Using `tensorrt` Python API:**

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np

class TRTInference:
    """TensorRT inference wrapper"""

    def __init__(self, engine_path):
        # Load TensorRT engine
        self.logger = trt.Logger(trt.Logger.WARNING)
        with open(engine_path, 'rb') as f:
            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())
        self.context = self.engine.create_execution_context()

        # Allocate GPU memory for inputs/outputs
        self.inputs = []
        self.outputs = []
        self.bindings = []

        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding))
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))

            # Allocate device memory
            device_mem = cuda.mem_alloc(size * np.dtype(dtype).itemsize)

            self.bindings.append(int(device_mem))

            if self.engine.binding_is_input(binding):
                self.inputs.append({'host': None, 'device': device_mem})
            else:
                host_mem = cuda.pagelocked_empty(size, dtype)
                self.outputs.append({'host': host_mem, 'device': device_mem})

    def infer(self, input_image):
        """Run inference on input image"""
        # Preprocessing: Resize to 640Ã—640, normalize, etc.
        input_data = preprocess(input_image)  # Your preprocessing function

        # Copy input to GPU
        cuda.memcpy_htod(self.inputs[0]['device'], input_data)

        # Run inference
        self.context.execute_v2(bindings=self.bindings)

        # Copy output from GPU
        for output in self.outputs:
            cuda.memcpy_dtoh(output['host'], output['device'])

        # Postprocessing: NMS, decode bounding boxes, etc.
        detections = postprocess(self.outputs[0]['host'])  # Your postprocessing

        return detections

# Usage
engine = TRTInference('yolov8n_fp16.trt')

import cv2
image = cv2.imread('test.jpg')

detections = engine.infer(image)
print(f"Detected {len(detections)} objects")
```

---

## INT8 Quantization

### Why INT8?

**Performance Gains:**
```
FP32 â†’ FP16: 2Ã— speedup
FP32 â†’ INT8: 4Ã— speedup âœ…
```

**Accuracy Impact:**
```
Model: YOLOv8n
FP32 mAP: 37.3%
FP16 mAP: 37.2% (0.1% loss)
INT8 mAP: 36.5% (0.8% loss) â† Acceptable!
```

---

### INT8 Calibration

**Challenge:** Converting FP32 weights (continuous) to INT8 (discrete) requires calibration to minimize accuracy loss.

**Process:**
```
1. Collect calibration dataset (100-500 representative images)
2. Run FP32 model on calibration data
3. Record activation histograms for each layer
4. Find optimal scaling factors (minimize quantization error)
5. Apply scaling to convert FP32 â†’ INT8
```

---

### Step-by-Step: INT8 Conversion

**Step 1: Prepare Calibration Dataset**

```python
import glob
import numpy as np

def load_calibration_data(calib_dir, max_images=500):
    """Load calibration images"""
    image_files = glob.glob(f"{calib_dir}/*.jpg")[:max_images]

    calib_data = []
    for img_path in image_files:
        img = cv2.imread(img_path)
        img = cv2.resize(img, (640, 640))
        img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]
        img = np.transpose(img, (2, 0, 1))  # HWC â†’ CHW
        calib_data.append(img)

    return np.array(calib_data)

calib_images = load_calibration_data('calib_images/', max_images=500)
np.save('calibration.npy', calib_images)
```

---

**Step 2: Create INT8 Calibrator**

```python
import tensorrt as trt

class Int8Calibrator(trt.IInt8EntropyCalibrator2):
    """INT8 calibrator for TensorRT"""

    def __init__(self, calib_data, cache_file='calibration.cache'):
        super().__init__()
        self.calib_data = calib_data
        self.cache_file = cache_file
        self.current_index = 0

        # Allocate GPU memory for calibration batch
        self.device_input = cuda.mem_alloc(calib_data[0].nbytes)

    def get_batch_size(self):
        return 1  # Process 1 image at a time

    def get_batch(self, names):
        if self.current_index < len(self.calib_data):
            batch = self.calib_data[self.current_index]
            cuda.memcpy_htod(self.device_input, batch)
            self.current_index += 1
            return [int(self.device_input)]
        else:
            return None  # End of calibration data

    def read_calibration_cache(self):
        # Return cached calibration if available
        if os.path.exists(self.cache_file):
            with open(self.cache_file, 'rb') as f:
                return f.read()
        return None

    def write_calibration_cache(self, cache):
        # Save calibration cache for future use
        with open(self.cache_file, 'wb') as f:
            f.write(cache)
```

---

**Step 3: Build INT8 Engine**

```bash
# Using trtexec with INT8 calibration
/usr/src/tensorrt/bin/trtexec \
    --onnx=yolov8n.onnx \
    --saveEngine=yolov8n_int8.trt \
    --int8 \
    --calib=calibration.cache \
    --workspace=4096 \
    --verbose

# Output:
# [I] Calibrating layer 'conv1' ...
# [I] Calibrating layer 'conv2' ...
# ...
# [I] Engine built in 120.5 seconds (longer due to calibration)
# [I] Average latency: 6.8 ms (vs 12.3 ms FP16! 45% faster!)
```

---

**Step 4: Validate INT8 Accuracy**

```python
# Compare FP32 vs INT8 on validation set
import torch
from ultralytics import YOLO

# FP32 PyTorch model
model_fp32 = YOLO('yolov8n.pt')
metrics_fp32 = model_fp32.val(data='coco128.yaml')

print(f"FP32 mAP@0.5: {metrics_fp32.box.map50:.3f}")

# INT8 TensorRT model (using validation script)
# ... (requires custom TensorRT inference + mAP calculation)
# Typically: INT8 mAP@0.5 â‰ˆ 0.365 (vs 0.373 FP32)

# Acceptable loss: < 2% relative (0.373 â†’ 0.365 = 2.1% loss âœ…)
```

---

## Dynamic Shapes

### The Problem

**Fixed Input Shape (Default TensorRT):**
```
Built engine with input shape: (1, 3, 640, 640)

Limitation: Can only process 640Ã—640 images!
If you resize to 1280Ã—1280 â†’ ERROR âŒ
```

---

### Solution: Dynamic Shapes

**Build Engine with Dynamic Shapes:**
```bash
/usr/src/tensorrt/bin/trtexec \
    --onnx=yolov8n.onnx \
    --saveEngine=yolov8n_dynamic.trt \
    --fp16 \
    --minShapes=input:1x3x320x320 \
    --optShapes=input:1x3x640x640 \
    --maxShapes=input:1x3x1280x1280 \
    --workspace=4096

# Now you can use any size from 320Ã—320 to 1280Ã—1280!
```

**Trade-off:** Slightly slower build time, but more flexible at inference.

---

### Using Dynamic Shapes in Python

```python
class TRTInferenceDynamic:
    """TensorRT inference with dynamic input shapes"""

    def __init__(self, engine_path):
        # ... (load engine same as before)
        self.context = self.engine.create_execution_context()

    def infer(self, input_image, input_size=(640, 640)):
        """Run inference with dynamic input size"""
        # Resize image to desired size
        input_data = cv2.resize(input_image, input_size)
        input_data = preprocess(input_data)  # Normalize, etc.

        # Set dynamic input shape
        self.context.set_binding_shape(0, (1, 3, input_size[0], input_size[1]))

        # Allocate memory based on new shape
        # ... (similar to before, but reallocate if shape changed)

        # Run inference
        self.context.execute_v2(bindings=self.bindings)

        return postprocess(self.outputs)

# Usage: Process images of different sizes
engine = TRTInferenceDynamic('yolov8n_dynamic.trt')

small_img = cv2.imread('small.jpg')
detections_small = engine.infer(small_img, input_size=(320, 320))  # Fast

large_img = cv2.imread('large.jpg')
detections_large = engine.infer(large_img, input_size=(1280, 1280))  # Slower, but more accurate
```

---

## Plugin Development (Advanced)

### When You Need Custom Layers

**Problem:** Your model has a custom operation not supported by TensorRT.

**Example:** Deformable convolution, RotatedRoIAlign, custom NMS

**Solution:** Write a TensorRT plugin (C++ CUDA code).

---

### Plugin Example: Custom ReLU6

**ReLU6:** `output = min(max(input, 0), 6)`

**C++ Plugin Code:**
```cpp
// relu6_plugin.cu
#include "NvInfer.h"
#include <cuda_runtime.h>

__global__ void relu6Kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = fminf(fmaxf(input[idx], 0.0f), 6.0f);
    }
}

class ReLU6Plugin : public nvinfer1::IPluginV2DynamicExt {
public:
    int enqueue(
        const nvinfer1::PluginTensorDesc* inputDesc,
        const nvinfer1::PluginTensorDesc* outputDesc,
        const void* const* inputs,
        void* const* outputs,
        void* workspace,
        cudaStream_t stream) override
    {
        int size = 1;
        for (int i = 0; i < inputDesc[0].dims.nbDims; i++) {
            size *= inputDesc[0].dims.d[i];
        }

        int threads = 256;
        int blocks = (size + threads - 1) / threads;

        relu6Kernel<<<blocks, threads, 0, stream>>>(
            static_cast<const float*>(inputs[0]),
            static_cast<float*>(outputs[0]),
            size
        );

        return 0;
    }

    // ... (other required methods: getOutputDimensions, etc.)
};

// Register plugin
REGISTER_TENSORRT_PLUGIN(ReLU6PluginCreator);
```

**Build and Use:**
```bash
# Compile plugin
nvcc -shared -o relu6_plugin.so relu6_plugin.cu -I/usr/include/x86_64-linux-gnu -lnvinfer

# Use in TensorRT
# Load plugin library in Python before building engine
import ctypes
ctypes.CDLL('relu6_plugin.so')

# Now TensorRT can parse models with ReLU6!
```

---

## Performance Benchmarking

### YOLOv8n on Jetson AGX Orin

| Precision | Latency (ms) | FPS | Memory (MB) | mAP@0.5 |
|-----------|--------------|-----|-------------|---------|
| **PyTorch FP32** | 45.2 | 22.1 | 150 | 37.3% |
| **TensorRT FP32** | 28.5 | 35.1 | 150 | 37.3% |
| **TensorRT FP16** | 12.3 | 81.3 | 75 | 37.2% |
| **TensorRT INT8** | 6.8 | 147 | 38 | 36.5% |

**Key Insights:**
- FP16: 3.7Ã— faster than PyTorch, no accuracy loss âœ…
- INT8: 6.6Ã— faster, only 0.8% mAP loss âœ… (Best for real-time robotics!)

---

### Batch Size Impact (INT8)

| Batch Size | Latency per Image (ms) | Throughput (FPS) | Memory (MB) |
|------------|------------------------|------------------|-------------|
| 1 | 6.8 | 147 | 38 |
| 2 | 5.2 | 192 (2Ã—96) | 55 |
| 4 | 4.3 | 232 (4Ã—58) | 90 |
| 8 | 3.9 | 205 (8Ã—25.6) | 160 |

**Sweet Spot:** Batch size 4 (best throughput-to-memory ratio).

**Use Case:** Process multiple camera streams simultaneously.

---

## Deployment Best Practices

### 1. Always Benchmark on Target Hardware

**Bad:**
```
Optimize on Desktop GPU (RTX 3090)
Deploy to Jetson Orin
Result: Suboptimal (different architecture!)
```

**Good:**
```
Profile on Jetson Orin directly
Optimize for Jetson-specific bottlenecks
Result: Maximum performance on target âœ…
```

---

### 2. Use Smaller Models When Possible

**Model Comparison (Same Task: Object Detection):**

| Model | Params | FP16 Latency | INT8 Latency | mAP@0.5 |
|-------|--------|--------------|--------------|---------|
| **YOLOv8n (Nano)** | 3M | 12ms | 7ms | 37.3% |
| **YOLOv8s (Small)** | 11M | 18ms | 10ms | 44.9% |
| **YOLOv8m (Medium)** | 26M | 35ms | 18ms | 50.2% |

**Decision:**
- If mAP > 40% is required â†’ Use YOLOv8s
- If speed is critical (>100 FPS) â†’ Use YOLOv8n + INT8

**Humanoid Example:** For gripper vision (close-range, simple objects), YOLOv8n is sufficient!

---

### 3. Warm Up the Engine

**First Inference is Slow:**
```python
# First inference: 50 ms (GPU kernels compile)
detections = engine.infer(image)

# Subsequent inferences: 7 ms (kernels cached)
detections = engine.infer(image2)
```

**Solution: Warm-Up Loop**
```python
# Warm up with dummy data
dummy_image = np.zeros((640, 640, 3), dtype=np.uint8)
for _ in range(10):
    engine.infer(dummy_image)

# Now real inferences are fast!
```

---

## Troubleshooting

### Issue 1: Engine Build Fails

**Error:**
```
[E] 3: [network.cpp::validate::2889] Error Code 3: API Usage Error (Parameter check failed at: ...
```

**Cause:** ONNX opset version incompatible with TensorRT.

**Fix:**
```python
# Export with specific opset version
model.export(format='onnx', opset=12)  # TensorRT supports opset 7-17
```

---

### Issue 2: Low INT8 Accuracy

**Symptom:** INT8 mAP drops > 3% (unacceptable).

**Cause:** Insufficient calibration data or poor data diversity.

**Fix:**
```python
# Increase calibration dataset size
calib_images = load_calibration_data('calib_images/', max_images=1000)  # Up from 500

# Ensure diversity (different lighting, objects, backgrounds)
```

---

### Issue 3: Slow Inference Despite TensorRT

**Symptom:** TensorRT INT8 is only 2Ã— faster than PyTorch (expected 4Ã—).

**Cause:** CPU-GPU synchronization overhead.

**Fix:**
```python
# Avoid cudaDeviceSynchronize() in hot path
# Use CUDA streams for async execution

stream = cuda.Stream()
self.context.execute_v2_async(bindings=self.bindings, stream_handle=stream.handle)
# Do other work here while GPU is busy...
stream.synchronize()  # Wait only when needed
```

---

## Summary

### Key Takeaways

1. âœ… **TensorRT optimizes models** through layer fusion, precision calibration, and kernel tuning
2. âœ… **FP16 provides 2Ã— speedup** with negligible accuracy loss
3. âœ… **INT8 provides 4Ã— speedup** with <2% accuracy loss (requires calibration)
4. âœ… **Dynamic shapes** enable flexible input sizes at inference
5. âœ… **Always benchmark on target hardware** (Jetson Orin, not desktop GPU)

---

### TensorRT Deployment Checklist

- [ ] Export PyTorch model to ONNX (opset 12-14)
- [ ] Build TensorRT engine with FP16 (baseline optimization)
- [ ] Prepare calibration dataset (500-1000 images)
- [ ] Build INT8 engine and validate accuracy (< 2% loss)
- [ ] Benchmark on Jetson Orin (latency, throughput, memory)
- [ ] Add warm-up loop before deployment
- [ ] Monitor inference time in production (detect degradation)

---

## Practice Problems

### Problem 1: Precision Selection

**Given:**
- Model: YOLOv8s
- Target FPS: 60
- Max acceptable accuracy loss: 1.5%

**Measured Performance:**
- FP32: 45 ms, mAP 44.9%
- FP16: 18 ms, mAP 44.7%
- INT8: 10 ms, mAP 43.8%

**Questions:**
1. Which precision meets the requirements?
2. What FPS can you achieve?

---

### Problem 2: Batch Processing

**Scenario:**
- Robot has 4 cameras (front, back, left, right)
- Each camera outputs 640Ã—640 images at 30 Hz
- Current: Process sequentially (28 ms per image Ã— 4 = 112 ms total)

**Question:**
1. Can batch processing help? (Assume batch-4 latency = 60 ms)
2. What throughput can you achieve?

---

## Further Reading

### Recommended Resources

**Official NVIDIA:**
- [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/)
- [TensorRT Python API](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/)
- [INT8 Calibration Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#working-with-int8)

**Tutorials:**
- [TensorRT Optimization for Jetson](https://developer.nvidia.com/blog/tensorrt-optimization-jetson/)
- [YOLOv8 TensorRT Deployment](https://github.com/ultralytics/ultralytics/blob/main/docs/guides/tensorrt.md)

---

## Next Chapter

Congratulations! You've completed **Chapter 2.3: Edge Compute Architecture**.

You now understand:
- âœ… NVIDIA Jetson hardware (CPU, GPU, unified memory)
- âœ… CUDA optimization techniques (coalescing, occupancy, fusion)
- âœ… TensorRT deployment (FP16/INT8 quantization, dynamic shapes)

**Next:** **Chapter 2.4: Power Management** - Learn how to design battery systems, thermal management, and power distribution for humanoid robots!

Continue to **[Chapter 2.4: Power Management](../power-management/2.4.1-battery-systems.md)** ðŸ”‹âš¡

---

**Section Status:** âœ… Complete
**Estimated Reading Time:** 50 minutes
**Hands-On Deployment:** 60 minutes (requires Jetson hardware or cloud GPU)
